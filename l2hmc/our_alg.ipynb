{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import random\n",
    "import pdb\n",
    "# from multiprocessing import Process, Manager\n",
    "# from multiprocessing import set_start_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/nkotelevskii/github/corrected_l2hmc/l2hmc_pytorch/l2hmc/utils/')\n",
    "from func_utils_pt import acl_spectrum, ESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributions_pt import Gaussian, GMM\n",
    "from layers_pt import Net\n",
    "from dynamics_pt import Dynamics\n",
    "from sampler_pt import propose\n",
    "from notebook_utils_pt import get_hmc_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 2  # dimensionality of data\n",
    "T = 5 # num of leapfrogs\n",
    "K = 10\n",
    "eps = 0.15\n",
    "\n",
    "mu = np.zeros(2,)\n",
    "mu = torch.tensor(mu, dtype=torch.float32).to(device)\n",
    "cov = np.array([[50.05, -49.95], [-49.95, 50.05]])\n",
    "cov = torch.tensor(cov, dtype=torch.float32).to(device)\n",
    "\n",
    "mu_1 = np.array([-2., 0.])\n",
    "mu_2 = np.array([2., 0.])\n",
    "mus = torch.tensor(np.array([mu_1, mu_2]), dtype=torch.float32, device=device)\n",
    "cov_1 = 0.1 * np.eye(2)\n",
    "cov_2 = 0.1 * np.eye(2)\n",
    "covs = torch.tensor(np.array([cov_1, cov_2]), dtype=torch.float32, device=device)\n",
    "pis = np.array([0.5, 0.5])\n",
    "\n",
    "\n",
    "# distribution = GMM(mus, covs, pis, device=device)\n",
    "distribution = Gaussian(mu, cov, device=device)\n",
    "\n",
    "dynamics = Dynamics(x_dim, distribution.get_energy_function(), T=T, eps=eps, net_factory=network, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = distribution.get_samples(200)\n",
    "plt.title('Strongly Correlated Gaussian (SCG)')\n",
    "plt.scatter(S[:, 0], S[:, 1])\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = 1000 #2500 #5000\n",
    "n_samples = 100\n",
    "# scale = torch.tensor(1., device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoregressive_coeff_logit = nn.Parameter(torch.tensor(0., device=device, dtype=torch.float32))\n",
    "params = list(dynamics.parameters()) + list([autoregressive_coeff_logit])\n",
    "optim = Adam(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = StepLR(optim, step_size=1000, gamma=0.96)\n",
    "std_normal = torch.distributions.Normal(loc=torch.tensor(0., dtype=torch.float32, device=device),\n",
    "                                                scale=torch.tensor(1., dtype=torch.float32, device=device))\n",
    "torch_log_2 = torch.tensor(np.log(2.), device=device, dtype=torch.float32)\n",
    "\n",
    "print_info = 50\n",
    "\n",
    "def compute_loss(z_new, p_new, z_old, p_old, sum_log_alpha, sum_log_jac):\n",
    "#     pdb.set_trace()\n",
    "    log_p = distribution.log_density(z_new) + std_normal.log_prob(p_new).sum(1)\n",
    "    log_r = -K * torch_log_2\n",
    "    log_m = std_normal.log_prob(z_old).sum(1) + std_normal.log_prob(p_old).sum(1) - sum_log_jac + sum_log_alpha\n",
    "    elbo_full = log_p + log_r - log_m\n",
    "    grad_elbo = torch.mean(elbo_full + elbo_full.detach() * sum_log_alpha)\n",
    "    return elbo_full.detach().mean().item(), grad_elbo\n",
    "\n",
    "for t in tqdm(range(n_batches)):    \n",
    "    plt.close()\n",
    "    z_old = std_normal.sample((n_samples, x_dim))\n",
    "    p_old = std_normal.sample((n_samples, x_dim))\n",
    "    optim.zero_grad()\n",
    "    sum_log_alpha = torch.zeros(z_old.shape[0], dtype=torch.float32, device=device) # for grad log alpha accumulation\n",
    "    sum_log_jacobian = torch.zeros(z_old.shape[0], dtype=torch.float32, device=device) # for log_jacobian accumulation\n",
    "    \n",
    "    p_new = p_old\n",
    "    z_new = z_old\n",
    "    \n",
    "    if t % print_info == 0:\n",
    "        array_z = []\n",
    "        array_directions = []\n",
    "        array_alpha = []\n",
    "        array_prop = []\n",
    "        \n",
    "    for k in range(K):\n",
    "        u = std_normal.sample(p_old.shape)\n",
    "#         pdb.set_trace()\n",
    "        z_prop, p_prop, log_alpha, z_new, log_jac, directions = propose(x=z_new, dynamics=dynamics,\n",
    "                                                       init_v=p_new, do_mh_step=True, device=device, our_alg=True, use_barker=True)\n",
    "        coeff = torch.sigmoid(autoregressive_coeff_logit)\n",
    "        p_new = p_prop * coeff + torch.sqrt(1. - coeff**2) * u\n",
    "        z_new = z_new[0]\n",
    "        \n",
    "        sum_log_alpha = sum_log_alpha + log_alpha\n",
    "        sum_log_jacobian = sum_log_jacobian + log_jac + x_dim * torch.log(coeff)\n",
    "        \n",
    "        if t % print_info == 0:\n",
    "            array_z.append(z_new.detach())\n",
    "            array_prop.append(z_prop.detach())\n",
    "            array_directions.append(directions.detach())\n",
    "            array_alpha.append(log_alpha.detach())\n",
    "            \n",
    "    if t % print_info == 0:\n",
    "        plt.scatter(S[:, 0], S[:, 1], label='Target')\n",
    "        plt.scatter(z_prop.cpu().detach()[:, 0], z_prop.cpu().detach()[:, 1], label='wo A/R')\n",
    "        plt.scatter(z_new.cpu().detach()[:, 0], z_new.cpu().detach()[:, 1], label='A/R')\n",
    "        plt.legend()\n",
    "        plt.show();\n",
    "        \n",
    "        fig, ax = plt.subplots(ncols=5, nrows=K // 5, figsize=(30, 10))\n",
    "        label = ['Same', 'Forward']\n",
    "        for kk in range(K):\n",
    "            ax[kk // 5, kk % 5].scatter(array_prop[kk][:, 0].cpu().numpy(), array_prop[kk][:, 1].cpu().numpy(), color='r', label='Proposals')\n",
    "            for d in [0., 1.]:\n",
    "                z_c = array_z[kk][array_directions[kk]==d]\n",
    "                alpha_c = array_alpha[kk][array_directions[kk]==d].cpu().exp().numpy()\n",
    "                color = np.zeros((z_c.shape[0], 4))\n",
    "                color[:, 3] = alpha_c\n",
    "                color[:, int(d + 1)] = 1.\n",
    "                ax[kk // 5, kk % 5].scatter(z_c[:, 0].cpu().numpy(), z_c[:, 1].cpu().numpy(), color=color, label=label[int(d)])\n",
    "#                 ax[kk].set_xlim((-x_lim, x_lim))\n",
    "#                 ax[kk].set_ylim((-y_lim, y_lim))\n",
    "                ax[kk // 5, kk % 5].legend()\n",
    "        plt.show();\n",
    "        \n",
    "        \n",
    "    elbo_full, grad_elbo = compute_loss(z_new=z_new, p_new=p_new, z_old=z_old, p_old=p_old, sum_log_alpha=sum_log_alpha,\n",
    "                                        sum_log_jac=sum_log_jacobian)\n",
    "    (-grad_elbo).backward()\n",
    "    optim.step()\n",
    "    scheduler.step()\n",
    "    #     pdb.set_trace()\n",
    "\n",
    "    if t % print_info == 0:\n",
    "        current_lr = None\n",
    "        for param_group in optim.param_groups:\n",
    "            current_lr = param_group['lr']\n",
    "        print ('Step: %d / %d, ELBO: %.2e, LR: %.5f, Autoregressive Coef %.2f, Step size %.2f' % (t, n_batches, elbo_full, current_lr,\n",
    "                                                            torch.sigmoid(autoregressive_coeff_logit).cpu().detach().item(),\n",
    "                                                              dynamics.alpha.cpu().detach().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_chains = 10000 #2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = torch.tensor(distribution.get_samples(n=n_samples), dtype=torch.float32, device=device)\n",
    "samples = std_normal.sample((n_samples, x_dim))\n",
    "p_new = std_normal.sample(samples.shape)\n",
    "final_samples = []\n",
    "samples_ = samples\n",
    "\n",
    "with torch.no_grad():\n",
    "    coeff = torch.sigmoid(autoregressive_coeff_logit)\n",
    "    for t in tqdm(range(length_of_chains)):\n",
    "        u = std_normal.sample(p_new.shape)\n",
    "        final_samples.append(samples_.cpu().numpy())\n",
    "        _, p_prop, _, z_new, _, _ = propose(x=samples_, dynamics=dynamics,\n",
    "                                                       init_v=p_new, do_mh_step=True, device=device, our_alg=True, use_barker=True)\n",
    "        p_new = p_prop * coeff + torch.sqrt(1. - coeff**2) * u\n",
    "        samples_ = z_new[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2HMC_samples = np.array(final_samples)\n",
    "with torch.no_grad():\n",
    "    HMC_samples_1 = get_hmc_samples(2, 0.1, distribution.get_energy_function(), steps=length_of_chains, samples=samples, device=device)\n",
    "    HMC_samples_2 = get_hmc_samples(2, 0.15, distribution.get_energy_function(), steps=length_of_chains, samples=samples, device=device)\n",
    "    HMC_samples_3 = get_hmc_samples(2, 0.2, distribution.get_energy_function(), steps=length_of_chains, samples=samples, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2HMC_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMC_samples_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = np.sqrt(np.trace(cov.cpu().detach().numpy()))\n",
    "L2HMC = acl_spectrum(L2HMC_samples, scale=scale)\n",
    "HMC1 = acl_spectrum(HMC_samples_1, scale=scale)\n",
    "HMC2 = acl_spectrum(HMC_samples_2, scale=scale)\n",
    "HMC3 = acl_spectrum(HMC_samples_3, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_to_plot = np.min([300, length_of_chains - 1])\n",
    "xaxis = 10 * np.arange(points_to_plot)\n",
    "plt.plot(xaxis, L2HMC[:points_to_plot], label='L2HMC')\n",
    "plt.plot(xaxis, HMC1[:points_to_plot], label='HMC $\\epsilon=0.1$')\n",
    "plt.plot(xaxis, HMC2[:points_to_plot], label='HMC $\\epsilon=0.15$')\n",
    "plt.plot(xaxis, HMC3[:points_to_plot], label='HMC $\\epsilon=0.2$')\n",
    "plt.ylabel('Auto-correlation')\n",
    "plt.xlabel('Gradient Computations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "\n",
    "points_to_plot = np.min([400, length_of_chains - 1])\n",
    "\n",
    "plt.scatter(S[:points_to_plot, 0], S[:points_to_plot, 1], label='True')\n",
    "plt.plot(L2HMC_samples[:points_to_plot, num, 0], L2HMC_samples[:points_to_plot, num, 1], label='Sampled L2HMC', color='black', marker='o')\n",
    "plt.plot(HMC_samples_2[:points_to_plot, num, 0], HMC_samples_2[:points_to_plot, num, 1], label='Sampled HMC $\\epsilon=0.1$', color='red', marker='s', alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 1\n",
    "plt.plot(L2HMC_samples[:points_to_plot, num, 0], L2HMC_samples[:points_to_plot, num, 1], label='Sampled L2HMC', color='black', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Condatorch",
   "language": "python",
   "name": "condatorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
